{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfQioNKroT0c"
      },
      "source": [
        "# Welcome!\n",
        "This notebook demonstrates how to develop a conversational system that uses a deep knowledge base about hotels, combining structured instance-level data and an ontological model. The knowledge graph (KG) and ontology enable reasoning to enhance dialogue response generation. The task involves integrating a GraphRAG-like approach to query the knowledge base and generate accurate, context-aware responses.\n",
        "\n",
        "Specifically, the notebook has the following steps:\n",
        "\n",
        "1. **Setup**: Loading the knowledge graph, dialogues, and required libraries (e.g., OWLAPY).\n",
        "2. **Analyzing the knowledge graph**: Exploring its structure and entities using OWLAPY queries.\n",
        "3. **Extending the ontology**: Adding TBox information for expressive reasoning.\n",
        "4. **Creating dialogues**: Create dialogues based on the examples. Write 5 simple dialogues and 5 more detailed ones to showcase different types of interactions.\n",
        "5. **Combining ontology and KG data**: Deploying an OWL reasoner to perform class-expression queries.\n",
        "6. **Query generation with LLMs**: Using an LLM (e.g., Llama3.2) to generate or assist in creating queries against the KG.\n",
        "7. **Generating responses**: Summarizing retrieved data into dialogue responses using a KG-augmented RAG approach.\n",
        "8. **Evaluation**: Assessing the system's performance using metrics like intersection-over-union scores.\n",
        "\n",
        "## Assignment\n",
        "The goal of this assignment is to develop a logic-enhanced conversational system that retrieves and reasons over domain knowledge to assist in dialogue response generation. You will focus on both the technical aspects of KG+ontology reasoning and the integration with LLMs for robust responses.\n",
        "\n",
        "### Assignment Steps\n",
        "1. **Analyze the provided knowledge graph and dialogues**:\n",
        "   - Explore the KG's entities, properties, and relevance to the dialogues.\n",
        "   - Identify opportunities where ontology reasoning enhances dialogue responses.\n",
        "2. **Extend the ontology**:\n",
        "   - Add expressive TBox information to support meaningful inferences.\n",
        "3. **Deploy the reasoning environment**:\n",
        "   - Use OWLAPY to combine the KG (as ABox) with the ontology for reasoning-based queries.\n",
        "4. **Generate class-expression queries**:\n",
        "   - Use instruction-based, few-shot prompting with Llama3.2 to produce or assist in creating the queries.\n",
        "5. **Summarize results into dialogue responses**:\n",
        "   - Apply KG-augmented RAG to generate user-facing answers based on reasoning results.\n",
        "6. **Evaluate the system**:\n",
        "   - Use appropriate metrics, including intersection-over-union scores for set-based answers.\n",
        "\n",
        "## Report\n",
        "Write a **5-page report** in LNCS format that includes:\n",
        "\n",
        "1. **Introduction**: Background on conversational systems with LLMs and the role of reasoning over domain knowledge.\n",
        "2. **Methodology**: A detailed description of your approach, including diagrams and examples.\n",
        "3. **Results**: Evaluation findings from the implemented steps.\n",
        "4. **Discussion**: Strengths and weaknesses of your approach, lessons learned, and potential improvements.\n",
        "\n",
        "Make sure to use the following template: [Springer Lecture Notes in Computer Science](https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science/kzwwpvhwnvfj)\n",
        "\n",
        "\n",
        "## Grading\n",
        "Your work will be evaluated based on:\n",
        "\n",
        "1. **Code Implementation (30%)**: Quality and functionality of the logic-enhanced conversational system.\n",
        "2. **Report (70%)**: Depth of analysis and clarity in presenting methods, results, and lessons learned.\n",
        "\n",
        "## Kaggle Environment Notes\n",
        "To ensure smooth execution:\n",
        "- Load the required data into `/kaggle/input/`.\n",
        "- Use `/kaggle/working/` for saving temporary files.\n",
        "- Turn on GPUs and internet connectivity when necessary, and follow best practices for resource management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-20T11:32:56.013831Z",
          "iopub.status.busy": "2024-12-20T11:32:56.013042Z",
          "iopub.status.idle": "2024-12-20T11:32:56.021502Z",
          "shell.execute_reply": "2024-12-20T11:32:56.020007Z",
          "shell.execute_reply.started": "2024-12-20T11:32:56.013767Z"
        },
        "id": "TV-bH-nEoT0h",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input director\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj0oLsVcoT0j"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzM11Swgj6lu"
      },
      "outputs": [],
      "source": [
        "!pip install jpype1==1.5.2\n",
        "!pip install owlapy==1.5.1\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9x785-oT0j"
      },
      "source": [
        "# Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSCgz1kEoT0j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from owlapy import manchester_to_owl_expression, dl_to_owl_expression\n",
        "from owlapy.iri import IRI\n",
        "from owlapy.owl_ontology import Ontology\n",
        "from owlapy.owl_reasoner import SyncReasoner, StructuralReasoner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deEK6s1KoT0k"
      },
      "source": [
        "# 1. Analyze the provided knowledge graph (data.ttl)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNliAm1VoT0k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO: your code to analyse the data.ttl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l59OpJZfoT0k"
      },
      "source": [
        "# 2. Create a small ontology that can support expressive inference about hotels and analyse the dialogues (examples.txt)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeFRzvfMoT0k"
      },
      "source": [
        "# Create your own dialogues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI6FQZTkoT0k"
      },
      "source": [
        "Once you have created your ontology, use it as the foundation for designing dialogues. Study the examples in examples.txt to understand their structure and content. Then, create 10 dialogues of your own, ensuring a range of difficulty levels: 5 simple ones and 5 more challenging ones. These dialogues should illustrate how your ontology can support reasoning and should include references to the types of information modeled in your ontology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPSzUBsDoT0k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create 10 dialoges based on the description\n",
        "dialogue1: str = \"\"\n",
        "dialogue2: str = \"\"\n",
        "dialogue3: str = \"\"\n",
        "dialogue4: str = \"\"\n",
        "dialogue5: str = \"\"\n",
        "dialogue6: str = \"\"\n",
        "dialogue7: str = \"\"\n",
        "dialogue8: str = \"\"\n",
        "dialogue9: str = \"\"\n",
        "dialogue10: str = \"\"\n",
        "dialogues: list = [dialogue1, dialogue2, dialogue3, dialogue4, dialogue5, dialogue6, dialogue7, dialogue8, dialogue9, dialogue10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr1iYN3doT0k"
      },
      "source": [
        "# 3. Deploy a reasoning environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLKQfS72pR6H"
      },
      "outputs": [],
      "source": [
        "ontology_path: str = \"...\" # your path (Kaggle, Colab or local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUnxbi69pR6H"
      },
      "outputs": [],
      "source": [
        "# TODO: load your ontology and create reasoner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xCMDIODoT0l"
      },
      "source": [
        "# 4. Instruct the LLM to produce the query or components of the query (e.g., keywords) against the KG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX5ofF4LoT0l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Download ollama\n",
        "# For Kaggle or Linux: download with this command, for Windows & Mac locally, download executable from website\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True) #runs on a different thread\n",
        "\n",
        "#Download Python library\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6_zA_R_pR6H"
      },
      "outputs": [],
      "source": [
        "# Import ollama & pull LLM\n",
        "import ollama\n",
        "!ollama pull llama3.2\n",
        "model: str = \"llama3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtGnyJ-2pR6H"
      },
      "outputs": [],
      "source": [
        "#Step 1: Write the instruction for the LLM - remember the overarching topic (assistance with hotels), as well as the fact that\n",
        "# this step is meant to merely extract queries from the user input.\n",
        "\n",
        "# Instruct LLM\n",
        "instruction: str = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMkVTdXvpR6H"
      },
      "outputs": [],
      "source": [
        "#Step 2: Write a function that takes the model, instruction and one user question as input, runs the LLM and outputs its response\n",
        "def question_to_query(instruction: str, question: str, model=\"llama3.2\") -> str:\n",
        "    '''\n",
        "    This function is meant to use the instruction defined above to run the LLM in order to convert one user input\n",
        "    question into a query for the ontology reasoner.\n",
        "    Parameters: instruction (string), question (string), model version (string)\n",
        "    Returns: LLM response (string)\n",
        "    '''\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THG7pHiBpR6I"
      },
      "outputs": [],
      "source": [
        "#Step 3: Run the LLM for each example defined above\n",
        "\n",
        "# Helper function\n",
        "def find_between(s: str, start: str, end: str) -> str:\n",
        "    return s.split(start)[1].split(end)[0]\n",
        "\n",
        "for dialogue in dialogues:\n",
        "    print(\"User question:\", dialogue)\n",
        "    print()\n",
        "    result: str = question_to_query(instruction, dialogue, model)\n",
        "    # Possibly only extract the relevant parts\n",
        "    print(\"Extracted query:\", result)\n",
        "    queries.append(result)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbGKo0GOoT0n"
      },
      "source": [
        "# 5. Use an LLM to summarize some result into a natural language response to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tGeVr3VpR6I"
      },
      "outputs": [],
      "source": [
        "#Step 1: Extract knowledge from query with the reasoner and return as list\n",
        "def reason(query: str) -> list:\n",
        "    '''\n",
        "    This function should convert a query into an OWL expression and use the reasoner\n",
        "    to return the answers.\n",
        "    '''\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BtRvMPlpR6I"
      },
      "outputs": [],
      "source": [
        "#Step 2: Instruct & run the LLM for the new task: transform the extracted knowledge into a natural language response based\n",
        "# on the original question\n",
        "def knowledge_to_response(question: str, knowledge: str, model=\"llama3.2\"):\n",
        "    '''\n",
        "    This function is meant to write an instruction based on an item of extracted knowledge and the original user\n",
        "    question, and run the LLM to summarize a response.\n",
        "    '''\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XLQA0EopR6I"
      },
      "outputs": [],
      "source": [
        "#Step 3: Combine everything: generate queries from the dialogues, extract knowledge from queries with the reasoner and\n",
        "# generate summary responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQrD2CWpR6I"
      },
      "source": [
        "# 6. Evaluate your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1RKysM3pR6I"
      },
      "outputs": [],
      "source": [
        "# TODO: your code to implement and demonstrate evaluation metrics\n",
        "# Suggestions: comparison of generated queries with the queries manually created in examples.txt, Intersection Over Union,\n",
        "# but you can be creative here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6343108,
          "sourceId": 10254373,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6343124,
          "sourceId": 10254398,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6343481,
          "sourceId": 10254852,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
